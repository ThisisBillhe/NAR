# Neighboring Autoregressive Modeling for Efficient Visual Generation


<div align="center">

[![arXiv](https://img.shields.io/badge/arXiv%20paper-2503.10696-b31b1b.svg)](https://arxiv.org/abs/2503.10696)&nbsp;
[![project page](https://img.shields.io/badge/Project_page-More_visualizations-green)](https://yuanyu0.github.io/nar/)&nbsp;

</div>


<p align="center">
<img src="assets/teaser.png" width=95%>
<p>

## Demo Video

<iframe width="560" height="315" src="https://www.youtube.com/embed/pnHADYVLuO4" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## ðŸ”¥ Update
- [2025.03.17] C2i, t2i, c2v training and sampling code are released !

## ðŸŒ¿ Introduction
We introduce NAR, a new "next-neighbor prediction" paradigm for efficient and high-quality visual generation. NAR achieves state-of-the-art generation quality and efficiency trade-off for both image and video generation tasks. All the training codes, data and models are open-sourced.

All pretrained models can be downloaded from [HuggingFace](https://huggingface.co/collections/chenfeng1271/nar-67d13fa93fe913b2e187ee1f).

## BibTeX
```bibtex
@article{he2025nar,
  title={Neighboring Autoregressive Modeling for Efficient Visual Generation},
  author={He, Yefei and He, Yuanyu and He, Shaoxuan and Chen, Feng and Zhou, Hong and Zhang, Kaipeng and Zhuang, Bohan},
  journal={arXiv preprint arXiv:2503.10696},
  year={2025}
}
```
